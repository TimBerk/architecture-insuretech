# Задание 1. Проектирование технологической архитектуры

Компания хочет сделать упор на развитие в регионах РФ. Планируется значительный рост количества пользователей и запросов. Нужно обеспечить бесперебойную работу сервиса 24/7, при этом сервис должен обслуживать клиентов из всех часовых поясов.

Требования к отказоустойчивости системы крайне высокие: RTO — 45 мин., RPO — 15 мин. Согласно требованиям бизнеса, доступность приложения должна быть равна 99,9%.

Дополнительно к этому нужно обеспечить одинаковое время загрузки страниц для пользователей из разных регионов. Оно не должно зависеть от географического местоположения пользователя.

На текущий момент сервис хранит ограниченный набор данных, который включает в себя:
* базовую информацию о клиентах — ФИО, контакты, документы,
* информацию о продуктах и тарифах,
* историю заявок клиентов.

Общий объём данных, которые хранятся в системе, равен 50 GB.

## Что нужно сделать

Необходимо спроектировать технологическую архитектуру приложения так, чтобы оно отвечало требованиям бизнеса. Создайте схему итогового решения на основании текущей технологической архитектуры сервиса.

Вот схема текущей архитектуры в draw.io:

<img src="https://pictures.s3.yandex.net/resources/78542_1726558806.png" />


Постарайтесь выдержать единый стиль оформления. Желательно, чтобы схема вашего решения была оформлена в той же нотации, что и схема текущей архитектуры. Для этого используйте библиотеки объектов Yandex Cloud. Ссылки на них мы дали в блоке «Как подготовиться к работе».

При проектировании уделите внимание следующим аспектам:

1. **Определите стратегию масштабирования и отказоустойчивости**. Рассмотрите вертикальное и горизонтальное масштабирование для вашей системы. Оцените, какая стратегия будет эффективнее. Требуется ли использование дополнительных зон безопасности?
2. Если приняли решение деплоить приложение в нескольких зонах безопасности, то продумайте и отразите на схеме следующие вопросы:
   * **Проработайте конфигурацию развёртывания приложения в Kubernetes**. Вы будете использовать независимые кластеры в каждой площадке или один растянутый? Оставьте на схеме комментарий с объяснением, почему вы выбрали тот или другой подход.
   * **Спланируйте балансировку нагрузки**. Опишите подход к балансировке нагрузки, который обеспечит распределение трафика между вашими сервисами и географически распредёленными серверами. Явно отразите на схеме все health check.
   * **Определите наиболее подходящую фейловер-стратегию**. Она должна отвечать заданным требованиям отказоустойчивости. Отразите её на схеме на уровне взаимодействия клиента с приложением.
   * **Определите конфигурацию базы данных**. Учитывая требования RTO и RPO, спроектируйте конфигурацию базы данных: определите, как вы будете обеспечивать репликацию данных и их резервное копирование. Если будете использовать конкретный фреймворк конфигурации кластера БД, отразите его на схеме.

3. Определите, будете ли вы применять шардирование БД. Отразите своё решение на схеме.

## Решение

<img src="./schema.png" />

### Масштабирование
- **Горизонтальное** для stateless‑сервисов (Controller и остальные API): HPA по CPU/RPS/очередям; это даёт и рост производительности, и устойчивость к падению отдельных Pod’ов/нод. 
- **Вертикальное** — точечно (например, тяжёлые воркеры), но как основная стратегия хуже для доступности и чаще приводит к рестартам/рескейджулингу. 
- Внутри каждого кластера обязательно распределение реплик по зонам/нодам через topology spread constraints, чтобы отказ зоны не “снял” все реплики. 

### Нужны ли дополнительные зоны безопасности

Для требований “пережить отказ зоны/площадки” используем multi‑AZ в пределах региона + **две площадки (Region A/B)** для DR. Kubernetes рассчитан на работу в нескольких зонах внутри региона. 

### Kubernetes: один растянутый или два независимых

**Два независимых кластера** `ks_a` и `ks_b`. Это снижает blast radius и упрощает эксплуатацию по сравнению со stretched‑кластером на большие расстояния/межрегиональной сети. 

### Балансировка и health checks

- GSLB на DNS: DNS отвечает адресом/набором адресов здоровых региональных LB (active-active с failover). 
- Региональный L7 LB проверяет `Ingress /healthz` и отправляет трафик только на здоровый ingress/backend. 

### Фейловер стратегия

- **Active-active**: обе площадки обслуживают трафик, DNS/health checks исключают недоступную площадку → минимальный RTO. 
- Вариант active-passive возможен ради экономии, но увеличивает RTO (нужно “разогреть” площадку). 

### База данных (RTO/RPO)

- Внутри региона: primary + replica (HA/чтение).  
- Для RPO: PITR через регулярные **base backups + WAL archive** в Object Storage (это снижает возможную потерю данных). 
- Для DR на уровне региона: копирование бэкапов/WAL во второй регион.
